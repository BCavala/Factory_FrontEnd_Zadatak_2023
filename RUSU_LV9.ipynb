{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7bd79e5206f14075821f7dba5344c501": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd0062000a0b4acf9bb01bdf922f6535",
              "IPY_MODEL_9ba21c4b0c3341a6b0eaa6186284c37f",
              "IPY_MODEL_9d44f883161447219a4b8241dba0ab78"
            ],
            "layout": "IPY_MODEL_d8d056781f174c0b9cae766d1ed68f1a"
          }
        },
        "cd0062000a0b4acf9bb01bdf922f6535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b2c01311fbc4fd8a5d5c2eac61e1bd7",
            "placeholder": "​",
            "style": "IPY_MODEL_4a5e2f48c2954679b3a59bddd2a3b430",
            "value": "100%"
          }
        },
        "9ba21c4b0c3341a6b0eaa6186284c37f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e902140dcb143ce96979aad18feda6b",
            "max": 200,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_428963fd950141988499692e6d8b59c0",
            "value": 200
          }
        },
        "9d44f883161447219a4b8241dba0ab78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbb19c81ee9e490c95839e6b8a3eedc9",
            "placeholder": "​",
            "style": "IPY_MODEL_7612533cc9dd48e08e33d2a94714f05c",
            "value": " 200/200 [00:00&lt;00:00, 534.73it/s]"
          }
        },
        "d8d056781f174c0b9cae766d1ed68f1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b2c01311fbc4fd8a5d5c2eac61e1bd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a5e2f48c2954679b3a59bddd2a3b430": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e902140dcb143ce96979aad18feda6b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "428963fd950141988499692e6d8b59c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dbb19c81ee9e490c95839e6b8a3eedc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7612533cc9dd48e08e33d2a94714f05c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BCavala/Factory_FrontEnd_Zadatak_2023/blob/master/RUSU_LV9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Podržano učenje\n",
        "\n",
        "Podržano učenje (*reinforcement learning*) je posebna vrsta strojnog učenja, različita od nadziranog i nenadziranog učenja. Za razliku od nadziranog i nenadziranog učenja, kod podržanog učenja nemamo podatkovni skup s ulaznim (i izlaznim) podacima, nego imamo nekakvo okruženje pomoću kojeg treniramo agenta.\n",
        "\n",
        "Agent se nalazi u nekakvom okruženju (*environment*) u kojem može obavljati određene akcije. U svakom trenutku je agent u određenom stanju. Nakon što agent obavi nekakvu akciju u određenom stanju, okruženje agenta stavlja u novo stanje i nagrađuje (ili kažnjava) ga za odrađenu akciju. U ovom LV-u ćemo promatrati slučaj u kojem je vrijeme diskretno, postoji ograničeni skup akcija i ograničeni skup stanja.\n",
        "\n",
        "Definiramo sljedeće izraze:\n",
        "- $t$ - vremenski trenutak\n",
        "- $a_t$ - akcija u trenutku $t$\n",
        "- $\\mathcal{A}$ - skup akcija\n",
        "- $s_t$ - stanje u trenutku $t$\n",
        "- $\\mathcal{S}$ - skup stanja\n",
        "- $r_t$ - nagrada u trenutku $t$\n",
        "- $a_{t+1}$ - akcija u trenutku $t+1$\n",
        "- $s_{t+1}$ - stanje u trenutku $t+1$\n",
        "- $s_{t+1}$ - nagrada u trenutku $t+1$\n",
        "\n",
        "Agent se u trenutku $t$ nalazi u stanju $s_t$ te obavlja akciju $a_t$. Okruženje ga zatim stavlja u stanje $s_{t+1}$ te mu daje nagradu $r_{t+1}$. Prikaz ovakvog sustava prikazan je na shemi.\n",
        "\n",
        "![reinforcement-learning.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "xADm5wBR8Niy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## *Q-learning*\n",
        "\n",
        "Q-learning je algoritam za učenje tzv. q-vrijednosti. q-vrijednost neke akcije $a$ u stanju $s$ je procjena nagrade akcije $a$ u stanju $s$. Ova procjena ne uključuje samo trenutnu nagradu nego i moguće buduće nagrade (kazne). Odnosno, možemo definirati funkciju Q:\n",
        "\n",
        "$$Q: \\mathcal{S} \\times \\mathcal{A} \\to \\mathbb{R}$$\n",
        "\n",
        "q-vrijednosti možemo promatrati kao tablicu gdje nam retci označavaju sva moguća stanja ($\\forall s \\in \\mathcal{S})$, a stupci sve moguće akcije ($\\forall a \\in \\mathcal{A}$).\n",
        "\n",
        "| -      | Action 1 ($a^1$) | Action 2 ($a^2$) | Action 3 ($a^3$) | Action 4 ($a^4$) |\n",
        "| ----------- | ----------- | ----------- | ----------- | ----------- |\n",
        "| State 1 ($s^1$) | Q($s^1, a^1$) | Q($s^1, a^2$) | Q($s^1, a^3$) | Q($s^1, a^4$) |\n",
        "| State 2 ($s^2$) | Q($s^2, a^1$) | Q($s^2, a^2$) | Q($s^2, a^3$) | Q($s^2, a^4$) |\n",
        "| ... | ... | ... | ... | ... |\n",
        "\n",
        "\n",
        "Prije svega inicijaliziramo q-vrijednosti na neku zadanu početnu vrijednost za sva stanja i sve akcije u tim stanjima. q-vrijednosti učimo na sljedeći način:\n",
        "\n",
        "1. Agent se u trenutku $t$ nalazi u stanju $s_t$. Odabire akciju $a_t$ koju će odraditi.\n",
        "2. Okruženje daje nagradu (ili kaznu) $r_{t+1}$ i stavlja agenta u stanje $s_{t+1}$\n",
        "3. $Q(s_t, a_t)$ se ažurira\n",
        "\n",
        "Ovaj proces ponavljamo dok agent ne dođe u neko konačno stanje, primjerice *smrt* u slučaju agenta u nekakvoj video igri. Ovo nazivamo epizodom te treniranje možemo vršiti proizvoljan broj epizoda."
      ],
      "metadata": {
        "id": "NhmpE8vOb__a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ažuriranje q-vrijednosti\n",
        "\n",
        "Posljednji korak u svakom koraku *Q-learninga* je ažurairanje q-vrijednosti. Ovo činimo pomoću Bellmanove jednadžbe:\n",
        "\n",
        "$$Q(s_t, a_t) \\leftarrow (1 - \\alpha)Q(s_t, a_t) + \\alpha(r_{t+1} + \\gamma \\max_a{Q(s_{t+1}, a)})$$\n",
        "\n",
        "Gdje su:\n",
        "- $\\alpha$ - stopa učenja (*learning rate*)\n",
        "- $\\gamma$ - faktor popusta (*discount factor*)\n",
        "- $r_{t+1}$ - nagrada dobivena prelaskom iz stanja $s_t$ u stanje $s_{t+1}$\n",
        "\n",
        "Tako se nova vrijednost $Q(s_t, a_t)$ definira preko tri faktora:\n",
        "- $(1 - \\alpha)Q(s_t, a_t)$ - trenutna vrijednost pomnožena s $(1 - \\alpha)$\n",
        "- $\\alpha r_{t+1}$ - nagrada koja se dobije u trenutku $t+1$ nakon što se u stanju $s_t$ učini akcija $a_t$ pomnožena s $\\alpha$\n",
        "- $\\alpha \\gamma \\max\\limits_a{Q(s_{t+1}, a)}$ - maksimalna q-vrijednost za stanje $s_{t+1}$ pomnožena s $\\alpha$ i $\\gamma$\n",
        "\n",
        "Što je $\\alpha$ veći, to će nove informacije više utjecati na novu q-vrijednost. $\\gamma$ definira koliko će buduće nagrade, odnosno q-vrijednosti, utjecati na trenutku q-vrijednost. Ako je $\\gamma = 0$, algoritam uopće neće uzimati buduće q-vrijednosti u obzir, odnosno bit će *kratkovidan*. Na rezultat *Q-learninga* također može utjecati i odabir inicijalnih q-vrijednosti."
      ],
      "metadata": {
        "id": "COrOqzKIoeVl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *Exploitation vs. exploration*\n",
        "\n",
        "Bitan dio *Q-learninga* je odabir akcije $a$ u stanju $s$. Očit način je jednostavan odabir akcije s najvećom q-vrijednošću u stanju $s$. Ovakav odabir akcije se naziva eksploatacija (*exploitation*), a takav algoritam bi onda bio pohlepan (*greedy*). Ovakav algoritam bi se potencijalno teško istrenirao jer bi u jednom trenutku isprobao *dobru* akciju i nikad više ne bi isprobao drugu akciju, a među drugim akcijama možda postoji još bolja akcija.\n",
        "\n",
        "Da bi se ovo izbjeglo, potrebno je uključiti i istraživanje (*exploration*) u algoritam. Jedan od načina da se ovo učini je da se doda vjerojatnost za odabir nasumične akcije, neovisno o q-vrijednostima. Ovaj pristup se naziva *$\\epsilon$-greedy* pristup i vjerojatnost za odabir nasumične akcije se označava s $\\epsilon$. Na ovaj način se može postići ravnoteža između eksploatacije i istraživanja.\n",
        "\n",
        "Teško je odrediti *ispravnu* vrijednost parametra $\\epsilon$, ali se u praksi često koriste vrijednosti oko $0.1$. Također je moguće prilagođavati vrijednost parametra $\\epsilon$ za vrijeme treniranja, pa primjerice početi s većom vrijednosti parametra $\\epsilon$ te ju s vremenom smanjivati i na kraju ju postaviti na 0 kako bi se algoritam *fine-tuneao* samo eksploatacijom."
      ],
      "metadata": {
        "id": "_OikneguuPns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nedostaci *Q-learninga*\n",
        "\n",
        "Postoje određeni problemi *Q-learninga*. Jedan od njih je već spomenut u potrebi ostaviravanja ravnoteže između eksploatacije i istraživanja.\n",
        "\n",
        "Također moguć problem je pristranost precjenjivanju (*overestimation bias*). Budući da su q-vrijednosti šumovite, uzimanjem maksimalne vrijednosti, dobit će precijenjena vrijednost. Uzmemo li primjerice diskretnu uniformnu razdiobu s vrijednostima 0 i 1, 2 puta izvučemo nasumičan broj iz razdiobe i odaberemo veći, dobit ćemo očekivanu vrijednost od $0.75$ za razliku od stvarne srednje vrijednosti te razdiobe koja je $0.5$. Jedan od načina rješavanja ovog problema je tzv. *double Q-learning*.\n",
        "\n",
        "Još jedan nedostatak *Q-learninga* je problem visoke dimenzionalnosti kada je moguć velik broj stanja ili akcija. Primjerice, ako nam je promatranje u obliku slike, vrlo je teško modelirati tablicu q-vrijednosti gdje su nam moguća stanja sve moguće vrijednosti promatrane slike. Čest način zaobilaženja ovog problema je upotrebom dubokog (*deep*) *Q-learninga*."
      ],
      "metadata": {
        "id": "t0JFAv5jy7nD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "OTvxEOcJsIqx"
      },
      "outputs": [],
      "source": [
        "## Importing useful libraries\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from imageio.v2 import imread\n",
        "%matplotlib inline\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from enum import Enum\n",
        "\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.animation as plt_animation\n",
        "from matplotlib import rc\n",
        "rc('animation', html='jshtml')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zmijica\n",
        "\n",
        "Za potrebe ove laboratorijske vježbe, napravljena je jednostavna video igra zmijica. Video igra ne može primati ulaze s računala, odnosno, može se igrati jedino preko poziva u kodu. Igra je implementirana u klasi `Snake` te su implementirane i pomoćna klasa `State` i [enumeracije](https://docs.python.org/3/library/enum.html) `Direction` i `Input`.\n",
        "\n",
        "`Direction` predstavlja moguće smjerove (i orijentacije) u koje zmijica može gledati, a oni su `UP, RIGHT, DOWN, LEFT`. Uz ovu enumeraciju, definiran je i *dictionary* `direction_dict` koji definira pomake (`[x, y]`) za svaki od smjerova (orijentacija).\n",
        "\n",
        "`Input` predstavlja moguće naredbe koje zmijica može primiti, a to su `CHILL` za nastavak pravocrtnog kretanja te `RIGHT` i `LEFT` za skretanje desno i lijevo za $90^\\circ$.\n",
        "\n",
        "Klasa `State` se koristi za praćenje stanja zmijice, a sadrži sljedeće atribute i funkcije:\n",
        "- `apple_front_back` - predstavlja je li jabuka ispred ili iza glave zmijice, ovisi o smjeru u kojem zmijica gleda i poprima sljedeće vrijednosti: (-1, jabuka je iza zmijice), (0, jabuka nije ni ispred ni iza zmijice), (1, jabuka je ispred zmijice)\n",
        "- `apple_left_right` - analogno `apple_front_back`, predstavlja je li jabuka lijevo ili desno od glave zmijice: (-1, jabuka je lijevo od zmijice), (0, jabuka nije ni lijevo ni desno od zmijice), (1, jabuka je desno od zmijice)\n",
        "- `danger` - lista koja predstavlja je li zmijica u neposrednoj opasnosti\n",
        "u sljedećim smjerovima `[ispred, desno, lijevo]`, 1 predstavlja da je neposredna opasnost u tom smjeru, a 0 da nije\n",
        "- `distance` - euklidska udaljenost između glave zmijice i jabuke\n",
        "- `dead` - je li zmijica mrtva\n",
        "- `ate` - je li zmijica upravo pojela jabuku\n",
        "- `get_state_str()` - ispis stanja zmijice bitan za q-vrijednosti, uključuje `apple_front_back`, `apple_left_right` i `danger`\n",
        "- `print_state()` - ispisuje stanje zmijice na čovjeku čitljiv način\n",
        "\n",
        "Također je dostupan i *dictionary* za boje."
      ],
      "metadata": {
        "id": "fGT9IPoJzCG1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sama klasa `Snake` u konstruktoru prima jedan parametar `board_size` koji definira veličinu polja u kvadratićima. Polje je kvadratnog oblika. Uz pomoćne funkcije, definirane su i funkcije `input`, `get_state` i `draw`.\n",
        "\n",
        "Funkcija `input` prima jedan parametar klase `Input` te obavlja sve što se dogodi prilikom jednog koraka igre: rotira zmijicu ukoliko je potrebno, pomiče zmijicu, detektira je li zmijica preminula, je li zmijica pojela jabuku te, ako je, generira novu jabuku i produljuje zmijicu.\n",
        "\n",
        "Funkcija `get_state` vraća trenutno stanje zmije u obliku objekta klase `State`. Funkcija `draw` vraća *sliku* trenutnog stanja igre, odnosno `numpy` polje koje se onda može crtati pomoću `matplotlib` biblioteke. Glavu zmijice crta zelenom bojom, tijelo plavom, a jabuku crvenom bojom. Ukoliko zmijica ugine, glava joj postaje tamno zelena."
      ],
      "metadata": {
        "id": "5a9f1bY-wp5s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enumeracije su u Pythonu poseban tip klasa u kojima definiramo neke atribute i njihove cjelobrojne vrijednsoti. Na ovaj način možemo osigurati jednostavnu pretvorbu između enumeracija i pripadnih cjelobrojnih vrijednosti.\n",
        "\n",
        "Možemo kreirati enumeraciju pomoću broja određene stavke. Primjerice, za kreiranje `DummyEnum.VAL0`, možemo pozvati `DummyEnum(0)`. Isto tako je moguće u suprotnom smjeru djelovati i jednostavno dobiti cjelobrojnu vrijednost određene stavke. Primjerice `DummyEnum.VAL0.value` će vratiti vrijednost `0`. Primjer rada s enumeracijom, prikazan je u sljedećoj ćeliji."
      ],
      "metadata": {
        "id": "tpJt7dn5wxJb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DummyEnum(Enum):\n",
        "  VAL0 = 0\n",
        "  VAL1 = 1\n",
        "  VAL2 = 2\n",
        "\n",
        "example_enum = DummyEnum.VAL1\n",
        "\n",
        "print('enum:            ', example_enum)\n",
        "\n",
        "print('enum value:      ', example_enum.value)\n",
        "\n",
        "example_enum_2 = DummyEnum((example_enum.value + 1) % 3)\n",
        "\n",
        "print('enum incremented:', example_enum_2)"
      ],
      "metadata": {
        "id": "A8IcIn_8w32j",
        "outputId": "bb3d1da2-6e30-4072-d5da-4439f6d7b81e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "enum:             DummyEnum.VAL1\n",
            "enum value:       1\n",
            "enum incremented: DummyEnum.VAL2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "colors = {\n",
        "  'white': [1, 1, 1],\n",
        "  'blue': [0, 0, 1],\n",
        "  'green': [0, 1, 0],\n",
        "  'red': [1, 0, 0],\n",
        "  'yellow': [1, 1, 0],\n",
        "  'dark_green': [0, 0.4, 0.2]\n",
        "}\n",
        "\n",
        "class Direction(Enum):\n",
        "  UP = 0\n",
        "  RIGHT = 1\n",
        "  DOWN = 2\n",
        "  LEFT = 3\n",
        "\n",
        "direction_dict = {\n",
        "  0: [-1, 0], ## UP\n",
        "  1: [0, 1],  ## RIGHT\n",
        "  2: [1, 0],  ## DOWN\n",
        "  3: [0, -1]  ## LEFT\n",
        "}\n",
        "\n",
        "class Input(Enum):\n",
        "  CHILL = 0\n",
        "  TURN_RIGHT = 1\n",
        "  TURN_LEFT = 2\n",
        "\n",
        "class State():\n",
        "  def __init__(self, apple_front_back, apple_left_right, danger, distance, dead, ate):\n",
        "    ## -1 for back, 0 for neither, 1 for front\n",
        "    ## up, down => state_1; left, right => state_2\n",
        "    self.apple_front_back = apple_front_back\n",
        "    ## -1 for left, 0 for neither, 1 for right\n",
        "    ## up, down => state_2; left, right => state_1\n",
        "    self.apple_left_right = apple_left_right\n",
        "    ## 0 no danger, 1 danger; [front, right, left]\n",
        "    self.danger = danger\n",
        "    self.distance = distance\n",
        "    self.dead = dead\n",
        "    self.ate = ate\n",
        "\n",
        "  def get_state_str(self):\n",
        "    return str([self.apple_front_back, self.apple_left_right, *self.danger])\n",
        "\n",
        "  def print_state(self):\n",
        "    print('state_str:', self.get_state_str())\n",
        "    print('distance:', self.distance)\n",
        "    print('dead:', self.dead)\n",
        "    print('ate:', self.ate)\n",
        "\n",
        "def array_in_list(arr, lst):\n",
        "  if np.any(np.all(arr == lst, axis=1)):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "class SnakeGame():\n",
        "  def __init__(self, board_size=10):\n",
        "    if board_size < 10:\n",
        "      board_size = 10\n",
        "\n",
        "    self.board_size = board_size\n",
        "    self.score = 0\n",
        "    self.dead = False\n",
        "    self.ate = False\n",
        "\n",
        "    ## Let the snake start somewhere near middle\n",
        "    self.head = np.random.randint(board_size // 2 - 2, board_size // 2 + 2, 2)\n",
        "\n",
        "    self.direction = Direction.RIGHT\n",
        "    self.snake = [self.head - self.direction_delta() - self.direction_delta() - self.direction_delta(),\n",
        "                  self.head - self.direction_delta() - self.direction_delta(),\n",
        "                  self.head - self.direction_delta(), self.head]\n",
        "\n",
        "    self.generate_apple()\n",
        "\n",
        "  def direction_delta(self):\n",
        "    return direction_dict[self.direction.value]\n",
        "\n",
        "  def generate_apple(self):\n",
        "    self.apple = np.random.randint(0, self.board_size, 2)\n",
        "    if array_in_list(self.apple, self.snake):\n",
        "      self.generate_apple()\n",
        "\n",
        "  def is_danger(self, new_loc):\n",
        "    '''\n",
        "    if snake will hit itself or if snake will go out of bounds\n",
        "    new_loc: location to check for danger\n",
        "    '''\n",
        "    return (array_in_list(new_loc, self.snake) or\n",
        "      np.any((new_loc < 0) | (new_loc >= self.board_size)))\n",
        "\n",
        "  def input(self, input):\n",
        "    self.ate = False\n",
        "    if self.dead:\n",
        "      return\n",
        "    if input == Input.TURN_RIGHT:\n",
        "      self.direction = Direction((self.direction.value + 1) % 4)\n",
        "    elif input == Input.TURN_LEFT:\n",
        "      self.direction = Direction((self.direction.value - 1) % 4)\n",
        "\n",
        "    new_head = self.head + self.direction_delta()\n",
        "    if self.is_danger(new_head):\n",
        "      # death\n",
        "      self.dead = True\n",
        "      return\n",
        "\n",
        "    self.head = new_head\n",
        "    self.snake.append(self.head)\n",
        "    if np.array_equal(self.head, self.apple):\n",
        "      self.generate_apple()\n",
        "      self.score += 1\n",
        "      self.ate = True\n",
        "    else:\n",
        "      self.snake.pop(0)\n",
        "\n",
        "  def get_state(self):\n",
        "    x_a, y_a = self.apple\n",
        "    x_h, y_h = self.head\n",
        "    direction = self.direction_delta()\n",
        "    ## non-zero direction\n",
        "    direction_1 = np.nonzero(direction)[0][0]\n",
        "    if x_a == x_h:\n",
        "      state_1 = 0\n",
        "    else:\n",
        "      state_1 = direction[direction_1] * (1 if int(x_a > x_h) else -1)\n",
        "    if y_a == y_h:\n",
        "      state_2 = 0\n",
        "    else:\n",
        "      state_2 = direction[direction_1] * (1 if int(y_a < y_h) else -1) * (-1 if direction_1 else 1)\n",
        "    ## -1 for back, 0 for neither, 1 for front\n",
        "    ## up, down => state_1; left, right => state_2\n",
        "    apple_front_back = state_2 if direction_1 else state_1\n",
        "    ## -1 for left, 0 for neither, 1 for right\n",
        "    ## up, down => state_2; left, right => state_1\n",
        "    apple_left_right = state_1 if direction_1 else state_2\n",
        "    ## 0 no danger, 1 danger; [front, right, left]\n",
        "    front = self.head + direction\n",
        "    right = self.head + direction_dict[(self.direction.value + 1) % 4]\n",
        "    left = self.head + direction_dict[(self.direction.value - 1) % 4]\n",
        "    danger = [int(self.is_danger(front)),\n",
        "              int(self.is_danger(right)),\n",
        "              int(self.is_danger(left))]\n",
        "\n",
        "    distance = np.linalg.norm(self.head - self.apple)\n",
        "\n",
        "    return State(apple_front_back, apple_left_right, danger, distance, self.dead, self.ate)\n",
        "\n",
        "  def draw(self):\n",
        "    self.board = np.ones((self.board_size, self.board_size, 3))\n",
        "    self.board[tuple(self.head)] = colors['dark_green'] if self.dead else colors['green']\n",
        "    for cell in self.snake[:-1]:\n",
        "      self.board[tuple(cell)] = colors['blue']\n",
        "    self.board[tuple(self.apple)] = colors['red']\n",
        "    return self.board.copy()"
      ],
      "metadata": {
        "id": "nZRxk00zQFTs"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = {\n",
        "  'white': [1, 1, 1],\n",
        "  'blue': [0, 0, 1],\n",
        "  'green': [0, 1, 0],\n",
        "  'red': [1, 0, 0],\n",
        "  'yellow': [1, 1, 0],\n",
        "  'dark_green': [0, 0.4, 0.2]\n",
        "}\n",
        "\n",
        "class Direction(Enum):\n",
        "  UP = 0\n",
        "  RIGHT = 1\n",
        "  DOWN = 2\n",
        "  LEFT = 3\n",
        "\n",
        "direction_dict = {\n",
        "  0: [-1, 0], ## UP\n",
        "  1: [0, 1],  ## RIGHT\n",
        "  2: [1, 0],  ## DOWN\n",
        "  3: [0, -1]  ## LEFT\n",
        "}\n",
        "\n",
        "class Input(Enum):\n",
        "  CHILL = 0\n",
        "  TURN_RIGHT = 1\n",
        "  TURN_LEFT = 2\n",
        "\n",
        "class State():\n",
        "  def __init__(self, apple_front_back, apple_left_right, danger, distance, dead, ate):\n",
        "    ## -1 for back, 0 for neither, 1 for front\n",
        "    ## up, down => state_1; left, right => state_2\n",
        "    self.apple_front_back = apple_front_back\n",
        "    ## -1 for left, 0 for neither, 1 for right\n",
        "    ## up, down => state_2; left, right => state_1\n",
        "    self.apple_left_right = apple_left_right\n",
        "    ## 0 no danger, 1 danger; [front, right, left]\n",
        "    self.danger = danger\n",
        "    self.distance = distance\n",
        "    self.dead = dead\n",
        "    self.ate = ate\n",
        "\n",
        "  def get_state_str(self):\n",
        "    return str([self.apple_front_back, self.apple_left_right, *self.danger])\n",
        "\n",
        "  def print_state(self):\n",
        "    print('state_str:', self.get_state_str())\n",
        "    print('distance:', self.distance)\n",
        "    print('dead:', self.dead)\n",
        "    print('ate:', self.ate)\n",
        "\n",
        "def array_in_list(arr, lst):\n",
        "  if np.any(np.all(arr == lst, axis=1)):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "class SnakeGame():\n",
        "  def __init__(self, board_size=10):\n",
        "    if board_size < 10:\n",
        "      board_size = 10\n",
        "\n",
        "    self.board_size = board_size\n",
        "    self.score = 0\n",
        "    self.dead = False\n",
        "    self.ate = False\n",
        "\n",
        "    ## Let the snake start somewhere near middle\n",
        "    self.head = np.random.randint(board_size // 2 - 2, board_size // 2 + 2, 2)\n",
        "\n",
        "    self.direction = Direction.RIGHT\n",
        "    self.snake = [self.head - self.direction_delta() - self.direction_delta() - self.direction_delta(),\n",
        "                  self.head - self.direction_delta() - self.direction_delta(),\n",
        "                  self.head - self.direction_delta(), self.head]\n",
        "\n",
        "    self.generate_apple()\n",
        "\n",
        "  def direction_delta(self):\n",
        "    return direction_dict[self.direction.value]\n",
        "\n",
        "  def generate_apple(self):\n",
        "    self.apple = np.random.randint(0, self.board_size, 2)\n",
        "    if array_in_list(self.apple, self.snake):\n",
        "      self.generate_apple()\n",
        "\n",
        "  def is_danger(self, new_loc):\n",
        "    '''\n",
        "    if snake will hit itself or if snake will go out of bounds\n",
        "    new_loc: location to check for danger\n",
        "    '''\n",
        "    return (array_in_list(new_loc, self.snake) or\n",
        "      np.any((new_loc < 0) | (new_loc >= self.board_size)))\n",
        "\n",
        "  def input(self, input):\n",
        "    self.ate = False\n",
        "    if self.dead:\n",
        "      return\n",
        "    if input == Input.TURN_RIGHT:\n",
        "      self.direction = Direction((self.direction.value + 1) % 4)\n",
        "    elif input == Input.TURN_LEFT:\n",
        "      self.direction = Direction((self.direction.value - 1) % 4)\n",
        "\n",
        "    new_head = self.head + self.direction_delta()\n",
        "    if self.is_danger(new_head):\n",
        "      # death\n",
        "      self.dead = True\n",
        "      return\n",
        "\n",
        "    self.head = new_head\n",
        "    self.snake.append(self.head)\n",
        "    if np.array_equal(self.head, self.apple):\n",
        "      self.generate_apple()\n",
        "      self.score += 1\n",
        "      self.ate = True\n",
        "    else:\n",
        "      self.snake.pop(0)\n",
        "\n",
        "  def get_state(self):\n",
        "    x_a, y_a = self.apple\n",
        "    x_h, y_h = self.head\n",
        "    direction = self.direction_delta()\n",
        "    ## non-zero direction\n",
        "    direction_1 = np.nonzero(direction)[0][0]\n",
        "    if x_a == x_h:\n",
        "      state_1 = 0\n",
        "    else:\n",
        "      state_1 = direction[direction_1] * (1 if int(x_a > x_h) else -1)\n",
        "    if y_a == y_h:\n",
        "      state_2 = 0\n",
        "    else:\n",
        "      state_2 = direction[direction_1] * (1 if int(y_a < y_h) else -1) * (-1 if direction_1 else 1)\n",
        "    ## -1 for back, 0 for neither, 1 for front\n",
        "    ## up, down => state_1; left, right => state_2\n",
        "    apple_front_back = state_2 if direction_1 else state_1\n",
        "    ## -1 for left, 0 for neither, 1 for right\n",
        "    ## up, down => state_2; left, right => state_1\n",
        "    apple_left_right = state_1 if direction_1 else state_2\n",
        "    ## 0 no danger, 1 danger; [front, right, left]\n",
        "    front = self.head + direction\n",
        "    right = self.head + direction_dict[(self.direction.value + 1) % 4]\n",
        "    left = self.head + direction_dict[(self.direction.value - 1) % 4]\n",
        "    danger = [int(self.is_danger(front)),\n",
        "              int(self.is_danger(right)),\n",
        "              int(self.is_danger(left))]\n",
        "\n",
        "    distance = np.linalg.norm(self.head - self.apple)\n",
        "\n",
        "    return State(apple_front_back, apple_left_right, danger, distance, self.dead, self.ate)\n",
        "\n",
        "  def draw(self):\n",
        "    self.board = np.ones((self.board_size, self.board_size, 3))\n",
        "    self.board[tuple(self.head)] = colors['dark_green'] if self.dead else colors['green']\n",
        "    for cell in self.snake[:-1]:\n",
        "      self.board[tuple(cell)] = colors['blue']\n",
        "    self.board[tuple(self.apple)] = colors['red']\n",
        "    return self.board.copy()"
      ],
      "metadata": {
        "id": "T3q-u5ydFyKO"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Play():\n",
        "  def __init__(self, board_size=10):\n",
        "\n",
        "    '''\n",
        "    board_size: defines the size of the board (the board is square)\n",
        "    --------------------------------------------------\n",
        "    qvalues: dictionary of the q-values where the keys are the states and\n",
        "              the values are the q-values for each of the actions\n",
        "    results: list of result dictionaries which have 'game_count' and 'score' keys\n",
        "    history: list of dictionaries which have 'state' and 'action' keys\n",
        "              which correspond to states and actions performed in those states\n",
        "    board_history: list of numpy arrays of the board state, these can be drawn using pyplot\n",
        "    animation: matplotlib.animation.FuncAnimation object with the animation of the whole game\n",
        "    '''\n",
        "\n",
        "    self.board_size = board_size\n",
        "\n",
        "    self.restart()\n",
        "    self.qvalues = {}\n",
        "\n",
        "    self.results = []\n",
        "\n",
        "  def train_qvalues_with_saving(self, num_episodes=10, epsilon=0.1, lr=0.7, discount=0.5,\n",
        "                                  rewards=[10, 5, -10, -2], remove_epsilon=100, starvation=50,\n",
        "                                  max_games=200, update_whole_history=False, verbose=False):\n",
        "        total_scores = []\n",
        "        average_scores = []\n",
        "\n",
        "        for episode in range(1, num_episodes + 1):\n",
        "            self.train_qvalues(epsilon, lr, discount, rewards, remove_epsilon,\n",
        "                               starvation, max_games, update_whole_history, verbose)\n",
        "\n",
        "            # Save Q-values after each episode\n",
        "            self.save_qvalues(episode)\n",
        "\n",
        "            # Load Q-values after each episode\n",
        "            self.load_qvalues(episode)\n",
        "\n",
        "            total_scores.append(self.game.score)\n",
        "            average_score = np.mean(total_scores)\n",
        "            average_scores.append(average_score)\n",
        "\n",
        "        # Display results on a graph\n",
        "        self.plot_results(num_episodes, average_scores)\n",
        "\n",
        "  def plot_results(self, num_episodes, average_scores):\n",
        "        # Plot the average scores over episodes\n",
        "        plt.plot(range(1, num_episodes + 1), average_scores, marker='o')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Average Score')\n",
        "        plt.title('Average Score Over Episodes')\n",
        "        plt.show()\n",
        "\n",
        "  def train_qvalues(self, epsilon=0.1, lr=0.7, discount=0.5,\n",
        "                    rewards = [10, 5, -10, -2], remove_epsilon=100, starvation=50,\n",
        "                    max_games=200, update_whole_history=False, verbose=True):\n",
        "    '''\n",
        "    train the q-values\n",
        "    epsilon: epsilon for the epsilon-greedy algorithm\n",
        "    lr: learning rate (alpha) for the Q-learning algorithm\n",
        "    discount: gamma parameter in the Bellman equation\n",
        "    rewards: a list of rewards in the following order: \\\n",
        "            [reward for eating an apple, reward for getting closer to the apple \\\n",
        "            reward for dying (negative), reward for getting further away from the apple(negative)]\n",
        "    remove_epsilon: number of games after which the epsilon should be set to equal 0\n",
        "    starvation: maximum amount of time without eating an apple before dying of starvation\n",
        "    max_games: number of games (episodes) to train for\n",
        "    update_whole_history: if True, updates the whole history of q-values at each iteration\n",
        "    verbose: if True, print the result of each game\n",
        "    '''\n",
        "    self.lr = lr\n",
        "    self.discount = discount\n",
        "    self.apple_reward, self.closer_reward, self.death_reward, self.further_reward = rewards\n",
        "    self.remove_epsilon = remove_epsilon\n",
        "    self.starvation = starvation\n",
        "    self.update_whole_history = update_whole_history\n",
        "\n",
        "    for game_count in tqdm(range(1, max_games + 1), position=0, leave=True):\n",
        "      ## restart the game\n",
        "      self.restart()\n",
        "      ## if the game_count is high enough, make epsilon = 0\n",
        "      if game_count > self.remove_epsilon:\n",
        "        self.epsilon = 0\n",
        "      else:\n",
        "        self.epsilon = epsilon\n",
        "      ## play until dead\n",
        "      self.play_game(train=True)\n",
        "\n",
        "      ## generate and store the results of the game for future reference\n",
        "      results = {\n",
        "          'game_count': game_count,\n",
        "          'score': self.game.score\n",
        "      }\n",
        "      self.results.append(results)\n",
        "\n",
        "      if verbose:\n",
        "        print(results)\n",
        "\n",
        "  def play_multiple(self, games=40):\n",
        "    '''\n",
        "    play multiple games and return the results of each of the games\n",
        "    games: number of games to play\n",
        "    '''\n",
        "    scores = []\n",
        "    for _ in tqdm(range(games)):\n",
        "      self.restart()\n",
        "      self.play_game(train=False)\n",
        "      scores.append(self.game.score)\n",
        "    return np.array(scores)\n",
        "\n",
        "  def play_game(self, train=True):\n",
        "    '''\n",
        "    play a game until the snake dies\n",
        "    train: if True, trains the q-values as well\n",
        "    '''\n",
        "    idle = 0\n",
        "    while not self.dead:\n",
        "      idle += 1\n",
        "      ## get an action and perform the action\n",
        "      action, state = self.get_action()\n",
        "      if state.ate:\n",
        "        idle = 0\n",
        "      self.perform_action(action)\n",
        "      if idle >= self.starvation:\n",
        "        self.dead = True\n",
        "      ## if the q-values are being trained, update them\n",
        "      if train:\n",
        "        self.update_qvalues(self.update_whole_history)\n",
        "\n",
        "  def play_dummy(self):\n",
        "    self.restart()\n",
        "    actions = [Input.CHILL, Input.TURN_RIGHT, Input.CHILL, Input.TURN_LEFT, Input.CHILL]\n",
        "    for action in actions:\n",
        "      self.perform_action(action)\n",
        "\n",
        "  def get_qvalues(self, state):\n",
        "    '''get the q-values for a given state'''\n",
        "    if state not in self.qvalues:\n",
        "      self.qvalues[state] = np.zeros(3)\n",
        "\n",
        "    return self.qvalues[state]\n",
        "\n",
        "  def print_qvalues(self):\n",
        "    '''get a pandas DataFrame of the q-values in a human readable format'''\n",
        "    return pd.DataFrame.from_dict(self.qvalues, orient='index')\n",
        "\n",
        "  def restart(self):\n",
        "    '''\n",
        "    restart the game\n",
        "    '''\n",
        "    self.game = SnakeGame(self.board_size)\n",
        "    self.dead = False\n",
        "    self.history = []\n",
        "\n",
        "    self.board_history = []\n",
        "    self.board_history.append(self.game.draw())\n",
        "\n",
        "  def get_action(self):\n",
        "    '''\n",
        "    get the action for the current state of the game using the epsilon greedy algorithm\n",
        "    if self.epsilon=0, returns the action with the highest q-value\n",
        "    also stores the action and the state into self.history list\n",
        "    '''\n",
        "    state = self.game.get_state()\n",
        "\n",
        "    ## epsilon greedy\n",
        "    rand = random.uniform(0, 1)\n",
        "    if rand < self.epsilon:\n",
        "        action = random.choice(list(Input))\n",
        "    else:\n",
        "        state_scores = self.get_qvalues(state.get_state_str())\n",
        "        action = Input(state_scores.argmax())\n",
        "\n",
        "    ## Remember the actions it took at each state\n",
        "    self.history.append({\n",
        "        'state': state,\n",
        "        'action': action\n",
        "        })\n",
        "    return action, state\n",
        "\n",
        "  def perform_action(self, input):\n",
        "    '''\n",
        "    if the snake is not dead, perform the input action\n",
        "    if the snake dies, set the score and prevent further play\n",
        "    also always add the board state array to the list\n",
        "    input: action the snake should take, Input object\n",
        "    '''\n",
        "    if not self.dead:\n",
        "      self.game.input(input)\n",
        "      ## if the snake died\n",
        "      if self.game.dead:\n",
        "        self.score = self.game.score\n",
        "        self.dead = True\n",
        "      self.board_history.append(self.game.draw())\n",
        "    return\n",
        "\n",
        "  def update_qvalue(self, state, action, future_state=None, death=False):\n",
        "    '''\n",
        "    update the q-values using the Bellman equation\n",
        "    if the snake has died, the last state-action q-value will be set\n",
        "    to equal the self.death_reward value\n",
        "    state: state of the last step\n",
        "    action: action of the last step\n",
        "    future_state: state after the action of the last step\n",
        "    death: bool value, true if the snake has died\n",
        "    '''\n",
        "    state_str = state.get_state_str()\n",
        "    action_value = action.value\n",
        "    if death:\n",
        "      ## TODO Implement the q-value update on death\n",
        "      ## q-value is equal to the death reward\n",
        "      ## there is no future state since game is over\n",
        "      self.get_qvalues(state_str)[action_value] = 0\n",
        "    else:\n",
        "      if future_state.ate: # Snake ate a food, positive reward\n",
        "        reward = self.apple_reward\n",
        "      elif future_state.distance < state.distance: # Snake is closer to the food, positive reward\n",
        "        reward = self.closer_reward\n",
        "      else:\n",
        "        reward = self.further_reward # Snake is further from the food, negative reward\n",
        "\n",
        "      future_state_str = future_state.get_state_str()\n",
        "\n",
        "      ## TODO Implement the Bellman equation\n",
        "      ## alpha - self.lr\n",
        "      ## gamma - self.discount\n",
        "      ## self.get_qvalues(state_string_) - gets the numpy array for the given state string\n",
        "      self.get_qvalues(state_str)[action_value] = 0 # Bellman equation\n",
        "\n",
        "  def update_qvalues(self, update_whole_history):\n",
        "    if update_whole_history:\n",
        "      self.update_qvalues_whole_history()\n",
        "    else:\n",
        "      self.update_current_qvalue()\n",
        "\n",
        "  def update_current_qvalue(self):\n",
        "    current_state = self.game.get_state() # current state\n",
        "    previous_state = self.history[-1]['state'] # previous state\n",
        "    previous_action = self.history[-1]['action'] # action taken at previous state\n",
        "\n",
        "    self.update_qvalue(previous_state, previous_action, current_state, self.game.dead)\n",
        "\n",
        "  def update_qvalues_whole_history(self):\n",
        "    '''\n",
        "    update the q-values for all the state-action pairs in the current episode\n",
        "    death: bool value, true if the snake has died\n",
        "    '''\n",
        "\n",
        "    ## reverse history, 0th element is the last step\n",
        "    history = self.history[::-1]\n",
        "    death = self.dead\n",
        "    for i, h in enumerate(history[:-1]):\n",
        "      if death: # Snake Died\n",
        "        state = history[0]['state']\n",
        "        action = history[0]['action']\n",
        "\n",
        "        self.update_qvalue(state, action, death=death)\n",
        "        death = False\n",
        "\n",
        "      else:\n",
        "        current_state = h['state'] # current state\n",
        "        previous_state = history[i+1]['state'] # previous state\n",
        "        previous_action = history[i+1]['action'] # action taken at previous state\n",
        "\n",
        "        self.update_qvalue(previous_state, previous_action, current_state)\n",
        "\n",
        "  def draw_history(self, save_path = None):\n",
        "    '''\n",
        "    draw the whole board_history as images and saves the animation\n",
        "    save_path: if not None, saves the animation as a gif with the given path\n",
        "    '''\n",
        "    fig, ax = plt.subplots()\n",
        "    im = ax.imshow(self.board_history[0])\n",
        "    ax.tick_params(\n",
        "      axis='both',\n",
        "      which='both',\n",
        "      bottom=False,\n",
        "      top=False,\n",
        "      left=False,\n",
        "      right=False,\n",
        "      labelbottom=False,\n",
        "      labelleft=False)\n",
        "    def update(i):\n",
        "      im.set_array(self.board_history[i])\n",
        "      return im,\n",
        "    animation_fig = plt_animation.FuncAnimation(fig, update, frames=len(self.board_history), interval=50, blit=True)\n",
        "\n",
        "    if save_path is not None:\n",
        "      animation_fig.save(save_path)\n",
        "\n",
        "    self.animation = animation_fig\n",
        "\n",
        "\n",
        "def update_qvalue(self, state, action, future_state=None, death=False):\n",
        "    state_str = state.get_state_str()\n",
        "    action_value = action.value\n",
        "    if death:\n",
        "        self.get_qvalues(state_str)[action_value] = self.death_reward  # Set to predefined death reward\n",
        "    else:\n",
        "        if future_state.ate:\n",
        "            reward = self.apple_reward\n",
        "        elif future_state.distance < state.distance:\n",
        "            reward = self.closer_reward\n",
        "        else:\n",
        "            reward = self.further_reward\n",
        "\n",
        "        future_state_str = future_state.get_state_str()\n",
        "\n",
        "        # Bellmanova jednadžba za ažuriranje Q-vrijednosti\n",
        "        alpha = self.lr\n",
        "        gamma = self.discount\n",
        "        current_qvalue = self.get_qvalues(state_str)[action_value]\n",
        "        future_max_qvalue = np.max(self.get_qvalues(future_state_str))\n",
        "        new_qvalue = (1 - alpha) * current_qvalue + alpha * (reward + gamma * future_max_qvalue)\n",
        "\n",
        "        self.get_qvalues(state_str)[action_value] = new_qvalue\n",
        "\n",
        "    def train_qvalues(self, epsilon=0.1, lr=0.7, discount=0.5,\n",
        "                      rewards=[10, 5, -10, -2], remove_epsilon=100, starvation=50,\n",
        "                      max_games=200, update_whole_history=False, verbose=True):\n",
        "        self.lr = lr\n",
        "        self.discount = discount\n",
        "        self.apple_reward, self.closer_reward, self.death_reward, self.further_reward = rewards\n",
        "        self.remove_epsilon = remove_epsilon\n",
        "        self.starvation = starvation\n",
        "        self.update_whole_history = update_whole_history\n",
        "\n",
        "        for game_count in tqdm(range(1, max_games + 1), position=0, leave=True):\n",
        "            self.restart()\n",
        "            if game_count > self.remove_epsilon:\n",
        "                self.epsilon = 0\n",
        "            else:\n",
        "                self.epsilon = epsilon\n",
        "\n",
        "            self.play_game(train=True)\n",
        "\n",
        "            results = {'game_count': game_count, 'score': self.game.score}\n",
        "            self.results.append(results)\n",
        "\n",
        "            if verbose:\n",
        "                print(results)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n4SjDUxepbxr"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from enum import Enum\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.animation as plt_animation\n",
        "\n",
        "class Input(Enum):\n",
        "    CHILL = 0\n",
        "    TURN_LEFT = 1\n",
        "    TURN_RIGHT = 2\n",
        "\n",
        "\n",
        "class Play():\n",
        "    def __init__(self, board_size=10):\n",
        "        self.board_size = board_size\n",
        "        self.restart()\n",
        "        self.qvalues = {}\n",
        "        self.results = []\n",
        "\n",
        "    def train_qvalues_with_saving(self, num_episodes=10, epsilon=0.1, lr=0.7, discount=0.5,\n",
        "                                  rewards=[10, 5, -10, -2], remove_epsilon=100, starvation=50,\n",
        "                                  max_games=200, update_whole_history=False, verbose=False):\n",
        "        total_scores = []\n",
        "        average_scores = []\n",
        "\n",
        "        for episode in range(1, num_episodes + 1):\n",
        "            self.train_qvalues(epsilon, lr, discount, rewards, remove_epsilon,\n",
        "                               starvation, max_games, update_whole_history, verbose)\n",
        "\n",
        "            # Save Q-values after each episode\n",
        "            self.save_qvalues(episode)\n",
        "\n",
        "            # Load Q-values after each episode\n",
        "            self.load_qvalues(episode)\n",
        "\n",
        "            total_scores.append(self.game.score)\n",
        "            average_score = np.mean(total_scores)\n",
        "            average_scores.append(average_score)\n",
        "\n",
        "        # Display results on a graph\n",
        "        self.plot_results(num_episodes, average_scores)\n",
        "\n",
        "    def plot_results(self, num_episodes, average_scores):\n",
        "        # Plot the average scores over episodes\n",
        "        plt.plot(range(1, num_episodes + 1), average_scores, marker='o')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Average Score')\n",
        "        plt.title('Average Score Over Episodes')\n",
        "        plt.show()\n",
        "\n",
        "    def train_qvalues(self, epsilon=0.1, lr=0.7, discount=0.5,\n",
        "                      rewards=[10, 5, -10, -2], remove_epsilon=100, starvation=50,\n",
        "                      max_games=200, update_whole_history=False, verbose=True):\n",
        "        self.lr = lr\n",
        "        self.discount = discount\n",
        "        self.apple_reward, self.closer_reward, self.death_reward, self.further_reward = rewards\n",
        "        self.remove_epsilon = remove_epsilon\n",
        "        self.starvation = starvation\n",
        "        self.update_whole_history = update_whole_history\n",
        "\n",
        "        for game_count in tqdm(range(1, max_games + 1), position=0, leave=True):\n",
        "            self.restart()\n",
        "            if game_count > self.remove_epsilon:\n",
        "                self.epsilon = 0\n",
        "            else:\n",
        "                self.epsilon = epsilon\n",
        "            self.play_game(train=True)\n",
        "\n",
        "            # Generate and store the results of the game for future reference\n",
        "            results = {\n",
        "                'game_count': game_count,\n",
        "                'score': self.game.score\n",
        "            }\n",
        "            self.results.append(results)\n",
        "\n",
        "            if verbose:\n",
        "                print(results)\n",
        "\n",
        "    def play_multiple(self, games=40):\n",
        "        scores = []\n",
        "        for _ in tqdm(range(games)):\n",
        "            self.restart()\n",
        "            self.play_game(train=False)\n",
        "            scores.append(self.game.score)\n",
        "        return np.array(scores)\n",
        "\n",
        "    def play_game(self, train=True):\n",
        "        idle = 0\n",
        "        while not self.dead:\n",
        "            idle += 1\n",
        "            action, state = self.get_action()\n",
        "            if state.ate:\n",
        "                idle = 0\n",
        "            self.perform_action(action)\n",
        "            if idle >= self.starvation:\n",
        "                self.dead = True\n",
        "            if train:\n",
        "                self.update_qvalues(self.update_whole_history)\n",
        "\n",
        "    def play_dummy(self):\n",
        "        self.restart()\n",
        "        actions = [Input.CHILL, Input.TURN_RIGHT, Input.CHILL, Input.TURN_LEFT, Input.CHILL]\n",
        "        for action in actions:\n",
        "            self.perform_action(action)\n",
        "\n",
        "    def get_qvalues(self, state):\n",
        "        if state not in self.qvalues:\n",
        "            self.qvalues[state] = np.zeros(3)\n",
        "        return self.qvalues[state]\n",
        "\n",
        "    def print_qvalues(self):\n",
        "        return pd.DataFrame.from_dict(self.qvalues, orient='index')\n",
        "\n",
        "    def restart(self):\n",
        "        self.game = SnakeGame(self.board_size)\n",
        "        self.dead = False\n",
        "        self.history = []\n",
        "\n",
        "        self.board_history = []\n",
        "        self.board_history.append(self.game.draw())\n",
        "\n",
        "    def get_action(self):\n",
        "        state = self.game.get_state()\n",
        "        rand = random.uniform(0, 1)\n",
        "        if rand < self.epsilon:\n",
        "            action = random.choice(list(Input))\n",
        "        else:\n",
        "            state_scores = self.get_qvalues(state.get_state_str())\n",
        "            action = Input(state_scores.argmax())\n",
        "\n",
        "        self.history.append({\n",
        "            'state': state,\n",
        "            'action': action\n",
        "        })\n",
        "        return action, state\n",
        "\n",
        "    def perform_action(self, input):\n",
        "        if not self.dead:\n",
        "            self.game.input(input)\n",
        "            if self.game.dead:\n",
        "                self.score = self.game.score\n",
        "                self.dead = True\n",
        "            self.board_history.append(self.game.draw())\n",
        "        return\n",
        "\n",
        "    def update_qvalue(self, state, action, future_state=None, death=False):\n",
        "        state_str = state.get_state_str()\n",
        "        action_value = action.value\n",
        "        if death:\n",
        "            self.get_qvalues(state_str)[action_value] = 0\n",
        "        else:\n",
        "            if future_state.ate:\n",
        "                reward = self.apple_reward\n",
        "            elif future_state.distance < state.distance:\n",
        "                reward = self.closer_reward\n",
        "            else:\n",
        "                reward = self.further_reward\n",
        "\n",
        "            future_state_str = future_state.get_state_str()\n",
        "\n",
        "            alpha = self.lr\n",
        "            gamma = self.discount\n",
        "            current_qvalue = self.get_qvalues(state_str)[action_value]\n",
        "            future_max_qvalue = np.max(self.get_qvalues(future_state_str))\n",
        "            new_qvalue = (1 - alpha) * current_qvalue + alpha * (reward + gamma * future_max_qvalue)\n",
        "\n",
        "            self.get_qvalues(state_str)[action_value] = new_qvalue\n",
        "\n",
        "    def update_qvalues(self, update_whole_history):\n",
        "        if update_whole_history:\n",
        "            self.update_qvalues_whole_history()\n",
        "        else:\n",
        "            self.update_current_qvalue()\n",
        "\n",
        "    def update_current_qvalue(self):\n",
        "        current_state = self.game.get_state()\n",
        "        previous_state = self.history[-1]['state']\n",
        "        previous_action = self.history[-1]['action']\n",
        "\n",
        "        self.update_qvalue(previous_state, previous_action, current_state, self.game.dead)\n",
        "\n",
        "    def update_qvalues_whole_history(self):\n",
        "        history = self.history[::-1]\n",
        "        death = self.dead\n",
        "        for i, h in enumerate(history[:-1]):\n",
        "            if death:\n",
        "                state = history[0]['state']\n",
        "                action = history[0]['action']\n",
        "                self.update_qvalue(state, action, death=death)\n",
        "                death = False\n",
        "            else:\n",
        "                current_state = h['state']\n",
        "                previous_state = history[i+1]['state']\n",
        "                previous_action = history[i+1]['action']\n",
        "                self.update_qvalue(previous_state, previous_action, current_state)\n",
        "\n",
        "    def draw_history(self, save_path=None):\n",
        "        fig, ax = plt.subplots()\n",
        "        im = ax.imshow(self.board_history[0])\n",
        "        ax.tick_params(\n",
        "            axis='both',\n",
        "            which='both',\n",
        "            bottom=False,\n",
        "            top=False,\n",
        "            left=False,\n",
        "            right=False,\n",
        "            labelbottom=False,\n",
        "            labelleft=False\n",
        "        )\n",
        "\n",
        "        def update(frame):\n",
        "            im.set_array(self.board_history[frame])\n",
        "            return [im]\n",
        "\n",
        "        ani = plt_animation.FuncAnimation(fig, update, frames=len(self.board_history), blit=True)\n",
        "        if save_path:\n",
        "            ani.save(save_path, writer='imagemagick', fps=2)\n",
        "        plt.show()\n",
        "\n",
        "    def save_qvalues(self, episode):\n",
        "        np.save(f'qvalues_episode_{episode}.npy', self.qvalues)\n",
        "\n",
        "    def load_qvalues(self, episode):\n",
        "        self.qvalues = np.load(f'qvalues_episode_{episode}.npy', allow_pickle=True).item()\n"
      ],
      "metadata": {
        "id": "SylSLHWSpf3A"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Zadaci\n",
        "\n",
        "1. Proučite klasu `Snake` i pomoćne klase, isprobajte rad klase kreiranjem objekta iste i odigrajte nekoliko poteza pomoću funkcije `input`. Između poteza ispišite stanje i iscrtajte polje za igru.\n",
        "\n",
        "2. Proučite klasu `Play`. Sve funkcije su opisane u kodu. Isprobajte funkcije `play_dummy` i `draw_history` kako biste dobili animaciju primjera igranja. Animaciju možete vizualizirati pristupanjem `animation` atributu klase `Play`. Također možete spremiti animaciju kao gif predajom putanje funkciji `draw_history`.\n",
        "\n",
        "3. Implementirajte ažuriranje q-vrijednosti u funkciji `update_qvalue`. Pokrenite *Q-learning* algoritam i istrenirajte q-vrijednosti pomoću funkcije `train_qvalues`. Proučite i odaberite prikladne parametre. Vizualizirajte posljednju epizodu igre.\n",
        "\n",
        "4. Korištenjem funkcije `play_multiple` odredite prosječni rezultat nakon treniranja.\n",
        "\n",
        "5. Dodajte funkcije za spremanje i učitavanje q-vrijednosti. Ponovno istrenirajte q-vrijednosti uz spremanje q-vrijednosti nakon svake epizode. Zatim učitajte q-vrijednosti nakon svake epizode i odredite prosječni rezultat za svaku epizodu treniranja. Prikažite dobivene rezultate na grafu.\n",
        "\n",
        "6. Ponovite 5. zadatak s različitim parametrima treniranja. Usporedite dobivene rezultate.\n",
        "\n",
        "7. Izmijenite klasu `State` i funkciju `get_state` klase `SnakeGame` kako biste obuhvatili više informaciju u stanju igre. Prilagodite i druge potrebne funkcije i klase te istrenirajte q-vrijednosti s novom definicijom stanja. Ponovite 5. zadatak za ovu definiciju stanja.\n",
        "\n",
        "Dodatno: Dodajte tekst za praćenje rezultata u animaciju igre. Dodajte vizualizaciju stanja igre u svakom trenutku u animaciju igre."
      ],
      "metadata": {
        "id": "uSHpDOD9zF7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the Play class\n",
        "snake_play = Play(board_size=10)\n",
        "\n",
        "# Play the game with the dummy actions\n",
        "snake_play.play_dummy()\n",
        "\n",
        "# Draw the history and display the animation (assuming you have matplotlib installed)\n",
        "snake_play.draw_history()\n"
      ],
      "metadata": {
        "id": "YflR5RVDpfGm",
        "outputId": "148705c2-bd83-4cc7-c030-6476e18eddb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play = Play(10)"
      ],
      "metadata": {
        "id": "uC9Zz6pDAHR7"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "play.play_dummy()"
      ],
      "metadata": {
        "id": "tCMSrlpncxWN"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "## prevent the output of the cell since the plt.imshow()\n",
        "## shows the image automatically\n",
        "play.draw_history('animation.gif')"
      ],
      "metadata": {
        "id": "bEjR1LcqRYkv",
        "outputId": "f0f41acf-55f6-4d31-ea5c-6c9f27cf3caf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.animation:MovieWriter imagemagick unavailable; using Pillow instead.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "play.animation"
      ],
      "metadata": {
        "id": "H7fH7WMlZuUe",
        "outputId": "7ddb30f0-ebde-4858-8339-aefd76e1f7db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Play' object has no attribute 'animation'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-fe1ebe393f23>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplay\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0manimation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'Play' object has no attribute 'animation'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the SnakeGame class\n",
        "snake_game = SnakeGame(board_size=10)\n",
        "\n",
        "# Play a few moves using the input function\n",
        "moves = [Input.TURN_RIGHT, Input.CHILL, Input.TURN_LEFT, Input.CHILL, Input.TURN_RIGHT]\n",
        "\n",
        "for move in moves:\n",
        "    # Perform the action\n",
        "    snake_game.input(move)\n",
        "\n",
        "    # Print the current state\n",
        "    print(\"Current State:\")\n",
        "    snake_game.get_state().print_state()\n",
        "\n",
        "    # Draw the game board\n",
        "    plt.imshow(snake_game.draw())\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "5SnMjDksGjlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the Play class\n",
        "play_instance = Play(board_size=10)\n",
        "\n",
        "# Play a dummy game\n",
        "play_instance.play_dummy()\n",
        "\n",
        "# Draw the history and save the animation as a gif\n",
        "play_instance.draw_history(save_path='snake_animation.gif')\n",
        "\n",
        "# Visualize the animation\n",
        "play_instance.animation\n"
      ],
      "metadata": {
        "id": "nvLibhiPp9xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kreirajte instancu klase Play\n",
        "play_instance = Play(board_size=10)\n",
        "\n",
        "# Trenirajte q-vrijednosti pomoću Q-learning algoritma\n",
        "play_instance.train_qvalues(epsilon=0.1, lr=0.7, discount=0.5, rewards=[10, 5, -10, -2],\n",
        "                            remove_epsilon=100, starvation=50, max_games=200, update_whole_history=False, verbose=True)\n",
        "\n",
        "# Vizualizirajte posljednju epizodu igre\n",
        "play_instance.draw_history(save_path='snake_game.gif')"
      ],
      "metadata": {
        "id": "7EYpTqmMqBR4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def play_multiple(play_instance, num_episodes=10):\n",
        "    total_scores = []\n",
        "\n",
        "    for _ in range(num_episodes):\n",
        "        play_instance.train_qvalues(epsilon=0.1, lr=0.7, discount=0.5, rewards=[10, 5, -10, -2],\n",
        "                                    remove_epsilon=100, starvation=50, max_games=200, update_whole_history=False, verbose=False)\n",
        "        total_scores.append(play_instance.game.score)\n",
        "\n",
        "    average_score = np.mean(total_scores)\n",
        "    print(f'Average Score over {num_episodes} episodes: {average_score}')\n",
        "\n",
        "# Kreirajte novu instancu klase Play\n",
        "play_instance_multiple = Play(board_size=10)\n",
        "\n",
        "# Pozovite funkciju play_multiple\n",
        "play_multiple(play_instance_multiple, num_episodes=10)\n"
      ],
      "metadata": {
        "id": "N0qdXjpBqNvM",
        "outputId": "869c5f50-7e03-4ea5-e9c7-e6d915210c5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:02<00:00, 98.75it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 80.75it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 87.43it/s]\n",
            "100%|██████████| 200/200 [00:05<00:00, 38.57it/s]\n",
            "100%|██████████| 200/200 [00:05<00:00, 35.31it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 67.71it/s]\n",
            "100%|██████████| 200/200 [00:03<00:00, 64.33it/s]\n",
            "100%|██████████| 200/200 [00:03<00:00, 66.61it/s]\n",
            "100%|██████████| 200/200 [00:05<00:00, 39.95it/s]\n",
            "100%|██████████| 200/200 [00:03<00:00, 53.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Score over 10 episodes: 5.4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of the Play class\n",
        "play_instance = Play()\n",
        "\n",
        "# Set hyperparameters\n",
        "num_episodes = 200\n",
        "epsilon = 0.1\n",
        "lr = 0.7\n",
        "discount = 0.5\n",
        "rewards = [10, 5, -10, -2]\n",
        "remove_epsilon = 100\n",
        "starvation = 50\n",
        "max_games = 200\n",
        "update_whole_history = False\n",
        "verbose = False\n",
        "\n",
        "# Train Q-values with saving and loading after each episode\n",
        "play_instance.train_qvalues_with_saving(num_episodes, epsilon, lr, discount, rewards,\n",
        "                                        remove_epsilon, starvation, max_games,\n",
        "                                        update_whole_history, verbose)"
      ],
      "metadata": {
        "id": "tXqXq8wYqQrZ",
        "outputId": "233a8f8d-e99c-4d72-d5a6-c808d337e4b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398,
          "referenced_widgets": [
            "7bd79e5206f14075821f7dba5344c501",
            "cd0062000a0b4acf9bb01bdf922f6535",
            "9ba21c4b0c3341a6b0eaa6186284c37f",
            "9d44f883161447219a4b8241dba0ab78",
            "d8d056781f174c0b9cae766d1ed68f1a",
            "0b2c01311fbc4fd8a5d5c2eac61e1bd7",
            "4a5e2f48c2954679b3a59bddd2a3b430",
            "1e902140dcb143ce96979aad18feda6b",
            "428963fd950141988499692e6d8b59c0",
            "dbb19c81ee9e490c95839e6b8a3eedc9",
            "7612533cc9dd48e08e33d2a94714f05c"
          ]
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/200 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7bd79e5206f14075821f7dba5344c501"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Play' object has no attribute 'save_qvalues'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-7230b1e29fc4>\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Train Q-values with saving and loading after each episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m play_instance.train_qvalues_with_saving(num_episodes, epsilon, lr, discount, rewards,\n\u001b[0m\u001b[1;32m     18\u001b[0m                                         \u001b[0mremove_epsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarvation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_games\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                                         update_whole_history, verbose)\n",
            "\u001b[0;32m<ipython-input-89-63c3afda3be2>\u001b[0m in \u001b[0;36mtrain_qvalues_with_saving\u001b[0;34m(self, num_episodes, epsilon, lr, discount, rewards, remove_epsilon, starvation, max_games, update_whole_history, verbose)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;31m# Save Q-values after each episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_qvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# Load Q-values after each episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Play' object has no attribute 'save_qvalues'"
          ]
        }
      ]
    }
  ]
}